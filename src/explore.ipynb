{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import zipfile\n",
                "import io\n",
                "\n",
                "url = \"https://storage.googleapis.com/datascience-materials/dogs-vs-cats.zip\"\n",
                "response = requests.get(url)\n",
                "response.raise_for_status()\n",
                "\n",
                "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
                "    z.extractall(\"dogs-vs-cats\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "\n",
                "images_folder = \"../data/dogs-vs-cats/dogs-vs-cats/train\"\n",
                "train_folder = \"../data/train\"\n",
                "cats_folder = \"../data/train/cats\"\n",
                "dogs_folder = \"../data/train/dogs\"\n",
                "\n",
                "os.makedirs(train_folder, exist_ok=True)\n",
                "os.makedirs(cats_folder, exist_ok=True)\n",
                "os.makedirs(dogs_folder, exist_ok=True)\n",
                "\n",
                "for filename in os.listdir(images_folder):\n",
                "    file_path = os.path.join(images_folder, filename)\n",
                "    if os.path.isfile(file_path):\n",
                "        if filename.lower().startswith(\"cat\"):\n",
                "            shutil.move(file_path, os.path.join(cats_folder, filename))\n",
                "        elif filename.lower().startswith(\"dog\"):\n",
                "            shutil.move(file_path, os.path.join(dogs_folder, filename))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Dividimos las imágenes en grupos de entrenamiento y pruebas**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import shutil\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "source_dir = \"../data/all\"\n",
                "\n",
                "train_dir = \"../data/train\"\n",
                "test_dir = \"../data/test\"\n",
                "\n",
                "os.makedirs(train_dir, exist_ok=True)\n",
                "os.makedirs(test_dir, exist_ok=True)\n",
                "\n",
                "classes = [\"cats\", \"dogs\"]\n",
                "\n",
                "for cls in classes:\n",
                "    cls_path = os.path.join(source_dir, cls)\n",
                "    files = os.listdir(cls_path)\n",
                "    \n",
                "    train_files, test_files = train_test_split(files, test_size=0.2, random_state=42)\n",
                "    \n",
                "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
                "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
                "    \n",
                "    for f in train_files:\n",
                "        shutil.copy(os.path.join(cls_path, f), os.path.join(train_dir, cls, f))\n",
                "    \n",
                "    for f in test_files:\n",
                "        shutil.copy(os.path.join(cls_path, f), os.path.join(test_dir, cls, f))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Cargamos las imágenes**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-20 17:28:34.855025: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
                        "2025-08-20 17:28:35.156554: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2025-08-20 17:28:59.229659: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2025-08-20 17:29:10.529308: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2025-08-20 17:29:10.530782: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
                    ]
                }
            ],
            "source": [
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_datagen = ImageDataGenerator()\n",
                "test_datagen = ImageDataGenerator()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_datagen_10_percent = ImageDataGenerator(rescale=1/255.)\n",
                "test_datagen_1_percent = ImageDataGenerator(rescale=1/255.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 20000 images belonging to 2 classes.\n",
                        "Found 5000 images belonging to 2 classes.\n"
                    ]
                }
            ],
            "source": [
                "train_data = train_datagen_10_percent.flow_from_directory(directory=train_dir,\n",
                "                                                          target_size=(224, 224),\n",
                "                                                          class_mode='categorical',\n",
                "                                                          batch_size=32,\n",
                "                                                          shuffle=True)\n",
                "\n",
                "test_data = test_datagen_1_percent.flow_from_directory(directory=test_dir,\n",
                "                                                          target_size=(224, 224),\n",
                "                                                          class_mode='categorical',\n",
                "                                                          batch_size=32,\n",
                "                                                          shuffle=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Construir un ANN**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.utils import set_random_seed\n",
                "\n",
                "set_random_seed(42)\n",
                "\n",
                "model = Sequential()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
                        "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
                        "2025-08-20 17:29:16.974102: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
                    ]
                }
            ],
            "source": [
                "model.add(Conv2D(input_shape = (224,224,3), filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 64,kernel_size = (3,3),padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = \"same\", activation = \"relu\"))\n",
                "model.add(MaxPool2D(pool_size = (2,2),strides = (2,2)))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(units = 4096,activation = \"relu\"))\n",
                "model.add(Dense(units = 4096,activation = \"relu\"))\n",
                "model.add(Dense(units = 2, activation = \"softmax\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Sequential name=sequential, built=True>"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-20 17:55:09.031008: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
                        "2025-08-20 17:55:09.031543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2025-08-20 17:55:09.081967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
                        "2025-08-20 17:55:12.131481: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2025-08-20 17:55:12.132350: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
                    ]
                }
            ],
            "source": [
                "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback  \n",
                "\n",
                "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)  \n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)  \n",
                "callbacks_list = [early_stop, reduce_lr]\n",
                "\n",
                "epochs = 5  \n",
                "batch_size = 32  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "20000"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "train_data.samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Entrenar el Modelo**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
                        "  self._warn_if_super_not_called()\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "model = model.fit(  \n",
                "    train_data,\n",
                "    epochs=epochs,\n",
                "    validation_data=test_data,\n",
                "    validation_steps=train_data.samples // batch_size,\n",
                "    callbacks=callbacks_list)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
